# k近邻法 (k-nearest neighbor, k-NN)

* Cover与Hart
  * 1968年提出$k$近邻法
* 策略
  * $k$近邻法假设给定一个训练数据集,其中的实例类别已定,分类时,对新的实例,根据其$k$个最近邻的训练实例类别,通过多数表决等方式进行预测
* $k$近邻算法 (k-nearest neighbor algorithm)
  * 训练数据集
    * $T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$ 
    * $x_i\in \cal X \rm\subseteq R^n$, $i=1,2,\cdots,N$, 为实例的特征向量
    * $y_i\in \cal Y\rm =\{c_1,c_2,\cdots,c_K\}$, 为实例的类别
  * (1) 根据给定的距离度量
    * 在训练集$T$中找出与$x$最邻近的$k$个点
    * 涵盖这$k$个点的$x$的邻域记作$N_k(x)$
  * (2) 在$N_k(x)$中根据分类决策规则决定$x$的类别$y$, (多数表决)
    * $y=arg\max\limits_{c_j}\sum\limits_{x_i\in N_k(x)}I(y_i=c_j)$ 
    * $i=1,2,\cdots,N$, $j=1,2,\cdots,K$
    * $I$为指示函数
      * 当$y_i=c_j$时$I$为1
      * 当$y_i\not=c_j$时$I$为0
  * 最近邻算法
    * $k$近邻法的特殊情况是$k=1$
    * 最近邻算法将训练数据集中与$x$最邻近的类作为$x$的类
* $k$近邻模型
  * 模型由三个基本要素构成
    * 距离度量
    * $k$值得选择
    * 分类决策规则
  * 距离度量
    * 设特征空间$\cal X$是$n$维实数向量空间$R^n$, $x_i,x_j\in \cal X$
      * $x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T$ 
      * $x_j=(x_j^{(1)},x_j^{(2)},\cdots,x_j^{(n)})^T$ 
    * $x_i,x_j$的$L_p$距离定义为
      * $L_p(x_i,x_j)=(\sum\limits_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}$ 
    * 欧氏距离(Euclidean distance)
      * 当$p=2$
        * $L_2(x_i,x_j)=(\sum\limits_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^2)^{\frac{1}{2}}$ 
    * 曼哈顿距离(Manhattan distance)
      * 当$p=1$
        * $L_1(x_i,x_j)=\sum\limits_{l=1}^n|x_i^{(l)}-x_j^{(l)}|$ 
    * 各个坐标距离的最大值
      * 当$p=+\infty$
        * $L_{\infty}(x_i,x_j)=\max\limits_l|x_i^{(l)}-x_j^{(l)}|$ 
  * $k$值得选择
    * $k$值的减小就意味着整体模型变得复杂,容易发生过拟合
    * $k​$值的增大就意味着整体模型变得简单
    * $k$值一般取一个比较小的数值,采用交叉验证法来选取最优的$k$值
  * 分类决策规则
    * 多数表决规则(majority voting rule)
      * 如果分类的损失函数为$0-1$损失函数
        * 分类函数为
          * $f:R^n\rightarrow\{c_1,c_2,\cdots,c_K\}$ 
        * 误分类的概率为
          * $P(Y\not=f(X))=1-P(Y=f(X))$ 
      * 对给定实例$x\in \cal X$,其最近邻的$k$个训练实例点构成集合$N_k(x)$,如果涵盖$N_k(x)$的区域的类别是$c_j$ 
        * 误分类率为
          * $\frac{1}{k}\sum\limits_{x_i\in N_k(x)}I(y_i\not=c_j)=1-\frac{1}{k}\sum\limits_{x_i\in N_k(x)}I(y_i=c_j)$ 
      * 要使误分类率最小即经验风险最小,就要使$\sum\limits_{x_i\in N_k(x)}I(y_i=c_j)$最大
        * 多数表决规则等价于经验风险最小化
  * $kd$树 ($kd$ tree)
    * 使用特殊结构存储训练数据,以减少计算距离的次数
    * $kd$树是一种对$k$维空间中的实例点进行存储以便对其进行快速检索的树形数据结构,$kd$树是二叉树,表示对$k$维空间的一个划分(partition)
    * 平衡$kd$树
      * $k$维空间数据集$T=\{x_1,x_2,\cdots,x_N\}$ 
        * $x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(k)})^T$, $i=1,2,\cdots,N$ 
      * (1) 构造根结点,根结点对应于包含T的k维空间的超矩形区域
        * 选择$x^{(1)}$为坐标轴,以T中所有实例的$x^{(1)}$坐标的中位数为切分点,将根结点对应的超矩形区域切分为两个子区域.切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现
        * 由根结点生成深度为1的左,右子结点
          * 左子结点对应坐标$x^{(1)}$小于切分点的子区域
          * 右子结点对应于坐标$x^{(1)}$大于切分点的子区域
        * 将落在切分超平面上的实例点保存在根结点
      * (2) 对深度为$j$的结点
        * 选择$x^{(l)}$为切分的坐标轴,$l＝j(mod k)+1$,以该结点的区域中所有实例的$x^{(l)}$坐标的中位数为切分点,将该结点对应的超矩形区域切分为两个子区域,切分由通过切分点并与坐标轴$x^{(l)}$垂直的超平面实现
        * 由该结点生成深度为$j+1$的左,右子结点
          * 左子结点对应坐标$x^{(l)}$小于切分点的子区域
          * 右子结点对应坐标$x^{(l)}$大于切分点的子区域
        * 将落在切分超平面上的实例点保存在该结点
      * (3) 直到两个子区域没有实例存在时停止
    * 搜索$kd$树
      * $kd$树的最近邻搜索
        * (1) 在$kd$树中找出包含目标点x的叶结点
          * 从根结点出发,递归地向下访问$kd$树
          * 若目标点$x$当前维的坐标小于切分点的坐标
            * 则移动到左子结点
            * 否则移动到右子结点
          * 直到子结点为叶结点为止
        * (2) 以此叶结点为"当前最近点"
        * (3) 递归地向上回退,在每个结点进行以下操作:
          * (a)如果该结点保存的实例点比当前最近点距离目标点更近
            * 则以该实例点为"当前最近点"
          * (b)当前最近点一定存在于该结点一个子结点对应的区域
            * 检查该子结点的父结点的另一子结点对应的区域是否有更近的点
              * 具体地,检查另一子结点对应的区域是否与以目标点为球心,以目标点与"当前最近点"间的距离为半径的超球体相交
              * 如果相交,可能在另一个子结点对应的区域内存在距目标点更近的点
              * 移动到另一个子结点,递归地进行最近邻搜索
              * 如果不相交则向上回退
        * (4)当回退到根结点时,搜索结束
          * 最后的"当前最近点"即为$x$的最近邻点
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T13:09:14.522881Z",
     "start_time": "2020-07-07T13:09:12.510172Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T13:09:15.544155Z",
     "start_time": "2020-07-07T13:09:15.528187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([6, 8, 10, 14, 18]).reshape(-1, 1)\n",
    "y = np.array([7, 9, 13, 17.5, 18])\n",
    "model=LinearRegression()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 残差平方和代价函数(损失函数)  \n",
    "$$RSS=\\sum\\limits_{i=1}^n(y_i-f(x_i))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T23:43:05.837542Z",
     "start_time": "2020-06-18T23:43:05.821876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7495689655172406"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RSS=np.mean((y-model.predict(X))**2)\n",
    "RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方差  \n",
    "方差用来衡量一组值的偏离程度,方差小意味着这组值都很接近总体的均值  \n",
    "n-1称为贝塞尔校正 : 纠正了对样本中总体方差估计的偏差  \n",
    "$$var(x)=\\frac{\\sum\\limits_{i=1}^n(x_i-\\overline{x})^2}{n-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T23:43:05.868713Z",
     "start_time": "2020-06-18T23:43:05.837542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23.2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance=((X-X.mean())**2).sum() / (X.shape[0]-1)\n",
    "print(variance)\n",
    "np.var(X,ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 协方差  \n",
    "协方差用来衡量两个变量如何变化  \n",
    "变量一起增加,它们的协方差为正  \n",
    "一个变量增加,另一个变量减少,它们的协方差为负  \n",
    "两个变量之间没有线性关系,它们的协方差为0 (线性无关,不一定是相对独立的)  \n",
    "$$cov(x,y)=\\frac{\\sum\\limits_{i=1}^n(x_i-\\overline{x})(y_i-\\overline{y})}{n-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T23:43:05.898569Z",
     "start_time": "2020-06-18T23:43:05.868713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22.650000000000002"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariance=np.multiply((X-X.mean()).transpose(),y-y.mean()).sum() / (X.shape[0]-1)\n",
    "print(covariance)\n",
    "np.cov(X.transpose(),y)[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 普通最小二乘OLS(Ordinary Lease Squares)  \n",
    "$$y=\\alpha+\\beta x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\beta=\\frac{cov(x,y)}{var(x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T23:43:05.914187Z",
     "start_time": "2020-06-18T23:43:05.898569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9762931034482758"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta=covariance/variance\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha=\\overline{y}-\\beta\\overline{x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T23:43:05.929942Z",
     "start_time": "2020-06-18T23:43:05.914187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9655172413793114"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()-beta*X.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R方(决定系数)  \n",
    "用来衡量数据和回归线的贴近程度  \n",
    "$$SS_{tot}=\\sum\\limits_{i=1}^n(y_i-\\overline{y})^2$$  \n",
    "$$SS_{res}=\\sum\\limits_{i=1}^n(y_i-f(x_i))^2$$  \n",
    "$$R^2=1-\\frac{SS_{res}}{SS_{tot}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T23:43:05.945579Z",
     "start_time": "2020-06-18T23:43:05.929942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9100015964240102"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SS_tot=((y-y.mean())**2).sum()\n",
    "SS_res=((y-model.predict(X))**2).sum()\n",
    "R_2=1-SS_res/SS_tot\n",
    "R_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T04:42:22.957897Z",
     "start_time": "2020-06-21T04:42:22.955432Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欧氏距离  \n",
    "$$d(p,q)=d(q,p)=\\sqrt{(q_1-p_1)^2+(q_2-p_2)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准确率 精准率 召回率\n",
    "TP : 真阳性的数量  \n",
    "FP : 假阳性的数量  \n",
    "TN : 真阴性的数量  \n",
    "FN : 假阴性的数量  \n",
    "准确率 : 衡量了分类器的整体正确性,但并不能区分假阳性错误,假阴性错误    \n",
    "$$ACC=\\frac{TP+TN}{TP+TN+FP+FN}$$  \n",
    "精准率 : 分类为阳性的真阳性的比例    \n",
    "$$P=\\frac{TP}{TP+FP}$$  \n",
    "召回率 : 真阳性被分类器辨认的比例    \n",
    "$$R=\\frac{TP}{TP+FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T23:43:05.976674Z",
     "start_time": "2020-06-18T23:43:05.945579Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-9-20c4bbb26899>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-20c4bbb26899>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    precision_score,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,\n",
    "                            precision_score,\n",
    "                            recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1得分(F1度量) 二元分类器性能衡量  \n",
    "F1得分 : 精准率和召回率的调和平均值  \n",
    "精准率与召回率的算数平均值是F1得分的上界  \n",
    "F1值会对精准率和召回率不平衡的分类器进行惩罚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T23:43:05.976674Z",
     "start_time": "2020-06-18T23:43:05.365Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCC马修斯相关系数  \n",
    "完美分类器的MCC=1  \n",
    "随机进行预测的分类器MCC=0  \n",
    "完全预测错误的分类器MCC=-1  \n",
    "测试数据类别比例非常不均衡时,MCC得分也非常有用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T23:43:05.976674Z",
     "start_time": "2020-06-18T23:43:05.379Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE&MSE  \n",
    "MSE MAE分别通过对误差求平方和求绝对值避免了回归模型中误差的方向导致的正负方向误差相互抵消的问题  \n",
    "MSE比MAE对异常值的惩罚程度要高 : 对一个较大的误差值求平方会加大它对整体误差的贡献比例  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 平均绝对误差MAE  \n",
    "预测结果误差绝对值的均值  \n",
    "$$MAE=\\frac{1}{n}\\sum\\limits_{i=0}^{n-1}|y_i-\\hat{y_i}|$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T04:41:25.804647Z",
     "start_time": "2020-06-21T04:41:25.801980Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 均方偏差MSE  \n",
    "$$MSE=\\frac{1}{n}\\sum\\limits_{i=0}^{n-1}(y_i-\\hat{y_i})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T04:41:43.220702Z",
     "start_time": "2020-06-21T04:41:43.217579Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot编码  \n",
    "类别变量 : 取值范围是一组固定的值  \n",
    "使用二进制特征表示解释变量的所有可能  \n",
    "将一个类别变量表示为多个二元特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T03:18:38.332020Z",
     "start_time": "2020-06-26T03:18:38.316360Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer # 对类别特征进行one-hot编码的转换器\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征标准化  \n",
    "标准化数据 : 零平均值和单位方差  \n",
    "零平均值解释变量相对于原点居中,其平均值为0  \n",
    "当特征向量所有特征的方差处于相同量级,则拥有单位方差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T03:05:02.519368Z",
     "start_time": "2020-06-20T03:05:02.516048Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欧几里得范数  \n",
    "两个向量之间的欧几里得距离等于两个向量差值的$L^2$范数  \n",
    "$$d=||x_0-x_1||$$  \n",
    "范数 : 为向量赋予正值尺寸的函数  \n",
    "一个向量的欧几里得范数等于这个向量的量级  \n",
    "$$||x||=\\sqrt{x_1^2+x_2^2+\\cdots+x_n^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T03:11:11.497048Z",
     "start_time": "2020-06-20T03:11:11.493565Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多元线性回归  \n",
    "$$Y=X\\beta$$  \n",
    "$$\\beta=(X^TX)^{-1}X^TY$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T03:22:16.462065Z",
     "start_time": "2020-06-20T03:22:16.458685Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import lstsq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多项式回归  \n",
    "解释变量和响应变量并不是线性关系  \n",
    "多项式回归 : 一种多元线性回归的特殊方式,用于对响应变量和多项式特征之间的关系进行建模  \n",
    "真实世界的曲线关系通过对特征做变换获得,而这些特征与多元线性回归的特征一致  \n",
    "$$y=\\alpha+\\beta_1 x+\\beta_2 x^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T03:41:37.907215Z",
     "start_time": "2020-06-20T03:41:37.903947Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures # 转换器 为一个特征表示增加多项式特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化  \n",
    "用于防止过拟合的技巧的集合  \n",
    "正则化为一个问题增加信息,通常是用一个对抗复杂度惩罚项的形式  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提克洛夫规范化(岭回归)  \n",
    "可以惩罚变大的模型参数  \n",
    "岭回归通过增加系数的$L^2$范数来修改RSS代价函数  \n",
    "$$RSS_{ridge}=\\sum\\limits_{i=1}^n(y_i-x_i^T\\beta)^2+\\lambda \\sum\\limits_{j=1}^p\\beta_j^2$$  \n",
    "$\\lambda$ : 控制惩罚力度的超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T04:53:46.376686Z",
     "start_time": "2020-06-20T04:53:46.372925Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO(最小绝对收缩和选择算子)  \n",
    "LASSO : 通过对代价函数增加$L^1$范数来惩罚系数  \n",
    "$$RSS_{lasso}=\\sum\\limits_{i=1}^n(y_i-x_i^T\\beta)^2+\\lambda \\sum\\limits_{j=1}^p|\\beta_j|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T04:57:30.286517Z",
     "start_time": "2020-06-20T04:57:30.283867Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 弹性网络正则化  \n",
    "是LASSO回归的$L^1$惩罚项和岭回归的$L^2$惩罚项的线性组合  \n",
    "也就是说,LASSO回归和岭回归都是弹性网络的特殊形式,其中$L^1$或$L^2$惩罚项对应的超参数分别等于0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T05:19:29.375573Z",
     "start_time": "2020-06-20T05:19:29.371584Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降法  \n",
    "一种能有效估计模型参数最优值的方法  \n",
    "梯度下降法根据每次训练迭代中用来更新模型参数的训练实例的数量不同  \n",
    "区分出3种变体:  \n",
    "批次梯度下降法 : 每次迭代中使用全部训练实例来更新模型参数  \n",
    "随机梯度下降法 : 每次迭代中仅仅使用一个训练实例来更新参数  \n",
    "小批次随机梯度下降法 : 每次迭代中使用包含数量b的小批次训练实例来跟新参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当有更多的训练实例时,小批次随机梯度下降法或随机梯度下降法会比批次梯度下降法收敛更快  \n",
    "批次梯度下降法是一种确定性算法,对于相同的训练数据集总产出同样的参数  \n",
    "随机梯度下降法可以在每次运行时产出不同的参数估计  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T06:24:49.276207Z",
     "start_time": "2020-06-20T06:24:49.272737Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor,SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正态分布  \n",
    "普通的线性回归假设响应变量符合正态分布  \n",
    "高斯分布 : 描述任何一个观测值对应一个位于两个实数之间值的概率的函数  \n",
    "正太分布数据是对称的,一半值大于均值,另一半值小于均值  \n",
    "正太分布数据的均值,中位数,众数相等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 伯努利分布  \n",
    "描述了一个只能取概率为P的正向情况或者概率为1-P的负向情况的随机变量的概率分布  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归  \n",
    "响应变量描述了结果是正向情况的概率  \n",
    "如果响应变量等于或者超出了一个区分阈值,则被预测为正向类,否则将被预测为负向类  \n",
    "响应变量使用逻辑函数建模为一个特征的线性组合函数  \n",
    "$$F(t)=\\frac{1}{1+e^{-t}}$$  \n",
    "对于逻辑回归,t等于解释变量的线性组合  \n",
    "$$F(x)=\\frac{1}{1+e{-(\\beta_0+\\beta x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 效用函数  \n",
    "效用函数 : 逻辑函数的逆,它将$F(x)$反连接到特征的一个线性组合  \n",
    "$$g(x)=\\ln\\frac{F(x)}{1-F(x)}=\\beta_0+\\beta x$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混淆矩阵  \n",
    "用来对真假阴阳性可视化  \n",
    "矩阵的行是实例的真实类  \n",
    "矩阵的列是实例的预测类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T12:15:00.050039Z",
     "start_time": "2020-06-20T12:15:00.046748Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC&AUC  \n",
    "受试者操作特征曲线(ROC) : 描绘了分类器召回率和衰退之间的关系,对类别分布不平衡的数据不敏感,表明了对所有阈值的性能    \n",
    "衰退(假阳性率) : 假阳性数量除以所有阴性数量的值  \n",
    "$$F=\\frac{FP}{FP+TN}$$  \n",
    "AUC : 将ROC曲线归纳为一个用来表示分类器预计性能的值,其值为ROC曲线下部分的面积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T13:22:41.660417Z",
     "start_time": "2020-06-20T13:22:41.654640Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网格搜索  \n",
    "产出最优模型超参数值的方法  \n",
    "接受一个包含所有应该被微调的超参数的可能的集合,并评估在该集合的卡氏积的每一个元素上训练的模型的性能(穷举搜索)  \n",
    "耗费算力,但可并行解决,进程间没有阻塞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T13:42:58.317469Z",
     "start_time": "2020-06-20T13:42:58.314220Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多类别分类  \n",
    "多类别分类问题的目标是将一个实例分配到类集合中的某一个  \n",
    "scikit-learn库使用  \n",
    "一对全 : 对每一个可能的类使用一个二元分类器    \n",
    "一对剩余 : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 汉明损失&杰卡德相似系数  \n",
    "多标签分类性能衡量指标  \n",
    "汉明损失 : 不正确标签的平均比例(损失函数,完美得分是0)  \n",
    "杰卡德指数 : 预测标签和真实标签交集的数量除以预测标签和真实标签并集的数量(取值0~1,1是完美得分)  \n",
    "$$J(Predict,True)=\\frac{Predict\\bigcap True}{Predict\\bigcup True}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T11:51:08.451194Z",
     "start_time": "2020-06-21T11:51:08.447631Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss,jaccard_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判别模型&生成模型  \n",
    "判别模型 : 学习一个决策边界对类进行判别  \n",
    "概率判别模型 : 如逻辑回归,会学习去估计条件概率P(y|x),就是说会根据给定的输入值去估计最有可能的类  \n",
    "非概率判别模型 : 如KNN,会直接把特征映射到类  \n",
    "生成模型 : 对特征和类的联合概率分布P(y,x)进行建模,就是说对类的概率和给定类的情况下特征的概率进行建模,生成概率对类如何生成特征进行建模  \n",
    "生成模型 : 如贝叶斯,估计给定特征的情况下,一个类的条件概率  \n",
    "生成模型比判别模型有更大的偏差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 独立同分布(.i.i.d)  \n",
    "训练实例相互独立,并且来源于同一个概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯  \n",
    "朴素假设 : 所有特征都相互 条件独立于其他给定的响应变量  \n",
    "贝叶斯定理 :   \n",
    "$$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$$\n",
    "特异性 : 衡量真阴性的比例,真正的阴性实例被预测为阴性的比例  \n",
    "$$\\frac{TN}{TN+FP}$$  \n",
    "一个阳性检验结果的概率等于真阳性和假阳性概率结果之和  \n",
    "$P(阳性)=P(阳性|患病)P(患病)+P(阳性|未患病)P(未患病)$  \n",
    "$$P(患病|阳性)=\\frac{P(阳性|患病)P(患病)}{P(阳性|患病)P(患病)+P(阳性|未患病)P(未患病)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类任务中  \n",
    "将解释变量的特征映射到一个离散的响应变量上,对于给定特征B,必须找出最有可能的标签A  \n",
    "$$P(y|x_1,\\cdots,x_n)=\\frac{P(x_1,\\cdots,x_n|y)P(y)}{P(x_1,\\cdots,x_n)}$$  \n",
    "y : 正向类  \n",
    "x : 特征  \n",
    "n : 特征数量  \n",
    "P(y) : 先验类概率(训练数据中每个类出现的频率)  \n",
    "$$\\hat{y}=arg\\max_yP(y)\\prod\\limits_{i=1}^nP(x_i|y)$$\n",
    "$P(x_1,\\cdots,x_n|y)$ : 条件概率(属于该类的训练实例特征的频率)\n",
    "$$\\hat{P}(x_i|y_i)=\\frac{N_{x_i,y_i}}{N_{y_i}}$$  \n",
    "分子 : 特征出现在y类训练实例中的次数  \n",
    "分母 : y类中所有特征出现的总频率  \n",
    "朴素贝叶斯通过极大化一个后验估计来估计先验概率与条件概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T12:56:31.420611Z",
     "start_time": "2020-06-21T12:56:31.417336Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB\n",
    "#高斯朴素贝叶斯 : 适合连续特征,假设每个特征对于每个类都符合正太分布\n",
    "#伯努利朴素贝叶斯 : 适合二元值特征\n",
    "#多项式朴素贝叶斯 : 适合类别特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 熵  \n",
    "熵的衡量方式来量化不确定性的程度,熵可以将一个变量中的不确定进行量化,并以比特为单位  \n",
    "$$H(X)=-\\sum\\limits_{i=1}^nP(x_i)\\log_bP(x_i)$$  \n",
    "n : 是结果的数量  \n",
    "$P(x_i)$ : 输出i的概率  \n",
    "b : 常见取值2,e,10  \n",
    "由于一个小于1的数值的对数为负数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信息增益  \n",
    "信息增益的指标来衡量熵的减少(表示父节点的熵和子节点的加权平均熵之间的差别)  \n",
    "$$IG(T,a)=H(T)-\\sum\\limits_{v\\in vals(a)}\\frac{|\\{x\\in T|x_a=v\\}|}{|T|}H(\\{x\\in T|x_a=v\\})$$  \n",
    "H(T) : 父节点的熵  \n",
    "T : 实例的集合  \n",
    "a : 检测中使用的特征  \n",
    "$X_a\\in vals(a)$ : 实例x对应的特征a的值  \n",
    "$X\\in T|X_a=v$ : 特征a的值等于v的实例数量  \n",
    "$H(X\\in T|X_a=v)$ : 特征a的值等于v的实例的子集的熵  \n",
    "父节点熵:0.9852 第一个子节点熵:0.7642 第二个子节点熵:0.7218 加权平均:0.7642*9/14+0.7219*5/14=0.7491 信息增益:0.9852-0.7491=0.2361"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基尼不纯度(启发式算法)  \n",
    "衡量一个集合中类的比例  \n",
    "$$Gini(t)=1-\\sum\\limits_{i=1}^jP(i|t)^2$$  \n",
    "j : 类的数量  \n",
    "t : 节点对应的实例子集  \n",
    "P(i|t) : 从节点的子集中选择一个属于类i元素的概率  \n",
    "当集合中所有元素都属于同一类时,由于选择一个元素属于这个类的概率都等于1,因此基尼不纯度等于0  \n",
    "和熵一样 : 当每个被选择的类概率都相等时基尼不纯度达到最大值  \n",
    "基尼不纯度的最大值依赖于其依赖的可能类的数量  \n",
    "$$Gini_{max}=1-\\frac{1}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树(勤奋学习模型)  \n",
    "高方差,低偏差  \n",
    "决策树学习算法可以从非均衡类比例的数据中产生有偏树  \n",
    "决策树容易过拟合 : 决策树学习算法会产出完美拟合每一个训练实例的巨型复杂的决策树模型,而无法对真实的关系进行泛化  \n",
    "剪枝 : 缓和决策树中的过拟合问题,它会移除决策树中一些过深的节点和叶子  \n",
    "预剪枝 : 通过设置决策树的最大深度,或者只在训练实例数量超出某个阈值时才创建子节点  \n",
    "集成 : 用于减少过拟合  \n",
    "ID3(贪婪算法) : 通过做出局部最优决策来高效地学习,但并不保证产出全局最优树  \n",
    "ID3算法通过选择一个特征序列进行测试来构造一颗树,每一个特征因其在一个节点中相比其他变量更能减少不确定性而被选择,为了找出全局最优树很有可能需要局部次优检测  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T06:22:40.593890Z",
     "start_time": "2020-06-26T06:22:40.589944Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集成算法  \n",
    "集成 : 指的是估计器的组合  \n",
    "套袋法  \n",
    "推进法  \n",
    "堆叠法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 套袋法(自发聚集)  \n",
    "是一种能减少一个估计器方差的集成元算法  \n",
    "用于分类任务和回归任务 : 当组件估计器为回归器时,集成将平均他们的预测结果,当组件估计器为分类器时,集成将返回模类  \n",
    "套袋法能在训练数据的变体上拟合多个模型(训练数据的变体使用一种称为自发重采样的流程来创建)  \n",
    "统计数值 : 通常来说,仅仅使用分布的一个样本来估计一个未知概率分布的参数是很有必要的,我们可以使用这个样本来计算一个统计数值,但是这个统计数值将会随我们恰巧取到的样本而变化  \n",
    "自发重采样 : 一种估计统计数值不确定性的方法,当且仅当样本中的观测值被独立地选取时,该方法才能被使用  \n",
    "通过重复地对原采样进行替换进行采样来产出采样的多个变体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T07:12:19.971345Z",
     "start_time": "2020-06-26T07:12:19.968353Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T07:20:54.287772Z",
     "start_time": "2020-06-26T07:20:54.282826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 23, 66, 67, 24, 52, 66, 35, 19, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#随机重采样\n",
    "sample=np.random.randint(1,100,10)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T07:15:02.704993Z",
     "start_time": "2020-06-26T07:15:02.698653Z"
    }
   },
   "outputs": [],
   "source": [
    "resample=[np.random.choice(sample,size=sample.shape) for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T07:16:56.662286Z",
     "start_time": "2020-06-26T07:16:56.656303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56.301000000000016"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(resample))\n",
    "resample_mean=np.array([x.mean() for x in resample])\n",
    "resample_mean.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T08:38:46.700695Z",
     "start_time": "2020-07-16T08:38:46.697200Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier,RandomForestClassifier,RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 推进法  \n",
    "用于减少一个估计器偏差的集成方法  \n",
    "AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T07:45:08.220320Z",
     "start_time": "2020-06-26T07:45:08.216763Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier,AdaBoostRegressor,GradientBoostingClassifier,GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 堆叠法(混合法)  \n",
    "创建集成的方法,使用一个元估计器去合并基础估计器的预测结果  \n",
    "基础估计器的预测结果和真实情况会作为元估计器的训练集  \n",
    "元估计器必须训练去使用基础估计器的预测结果来预测响应变量的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T08:14:07.466476Z",
     "start_time": "2020-06-26T08:14:07.454906Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone,BaseEstimator,TransformerMixin,ClassifierMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数  \n",
    "感知机通过激活函数处理特征和模型参数的线性组合来对实例进行分类  \n",
    "特征和模型参数的线性组合也被称作预激活  \n",
    "$$y=\\Phi(\\sum\\limits_{i=1}^nw_ix_i+b)$$  \n",
    "$\\Phi$ : 激活函数  \n",
    "$w_i$ : 模型参数  \n",
    "b : 误差项常数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单元阶跃函数(海维赛德阶跃函数)  \n",
    "$$g(x)=\\begin{cases} 1,& x>0\\\\0,& 其他\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 逻辑S型曲线(可导函数)  \n",
    "阶跃函数的光滑版本,它在极值区间内逼近一个阶跃函数,但是可以输出0~1之间的任何值\n",
    "$$g(x)=\\frac{1}{1+e^{-x}}$$  \n",
    "x : 输入项的权重求和结果  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机(误差驱动学习算法)  \n",
    "权重更新公式  \n",
    "$$w_i(t+1)=w_i(t)+\\alpha(true_j-predict_j(t))x_{j,i}$$  \n",
    "$\\alpha$ : 超参数学习率  \n",
    "j : 实例j  \n",
    "i : 实例j的第i个特征  \n",
    "权重参数向实例正确分类的方向进行调整  \n",
    "epoch : 每遍历一遍所有的训练实例称之为一个训练周期  \n",
    "收敛 : 如果学习算法在一个训练周期内对所有的训练实例分类正确,则达到收敛状态  \n",
    "学习算法并非一定能保证能够收敛,所以学习算法需要一个超参数来指定算法终止前能够完成的最大训练周期数  \n",
    "感知机通过处理实例特征和权重系数的线性组合,并根据激活函数的输出结果来对实例进行分类  \n",
    "使用逻辑S型曲线激活函数的感知机模型和逻辑回归模型相同,但感知机在学习其权重系数时使用了一种在线的,误差驱动算法  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T14:15:34.245539Z",
     "start_time": "2020-06-26T14:15:34.241540Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 支持向量机SVM(判别模型)  \n",
    "使用线性模型逼近非线性模型时,增加特征空间的维度虽然是一种有效的技巧  \n",
    "计算问题 : 计算映射特征和计算更大的向量会需要更多的算力  \n",
    "模型泛化能力 : 增加特征表示的维度会加剧维度诅咒的程度(为了避免拟合,从高维度特征表示中学习需要的训练数据将会呈指数增加)  \n",
    "使用SVM时对特征缩放是很重要的  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对偶形式  \n",
    "$$f(x)=<w,x>+b=\\sum\\alpha_iy_i<x_i,x>+b$$  \n",
    "原始形式计算了模型参数和测试实例特征向量的内积  \n",
    "对偶形式计算了训练实例和测试实例特征向量的内积  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 核  \n",
    "核是这样一个函数 : 只要给定了原始特征向量,就能返回和其相关的映射特征向量相同的点积值  \n",
    "$$K(x,z)=<\\phi(x),\\phi(z)>$$  \n",
    "$$x=(x_1,x_2)$$  \n",
    "$$z=(z_1,z_2)$$  \n",
    "$$<\\phi(x),\\phi(z)>=<(x_1^2,x_2^2\\sqrt{2}x_1x_2),(z_1^2,z_2^2\\sqrt{2}z_1z_2)>$$  \n",
    "$$K(x,z)=<x,z>^2=(x_1z_1+x_2z_2)^2=x_1^2z_1^2+2x_1z_1x_2z_2+x_2^2z_2^2$$  \n",
    "常用的核函数 : 多项式核 S型核 高斯核 线性核  \n",
    "多项式核 : $K(x,x')=(\\gamma<x-x'>+r)^k$(k=2平方核,常被用于自然语言处理)  \n",
    "S型核 : $K(x,x')=\\tanh(\\gamma<x-x'>+r)$  \n",
    "高斯核(径向基函数) : $K(x,x')=\\exp(-\\gamma|x-x'|^2)$ (高斯核产出的特征空间可以拥有无限维度)  \n",
    "核函数能和任何能够被表示为两个特征向量点积的模型一起使用(逻辑回归 感知机 主成分分析)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 函数间隔  \n",
    "评估预测的可信度 : 实例越靠近决策边界被预测为正向类的概率机会越低  \n",
    "$$funct=\\min y_if(x_i)$$  \n",
    "$$f(x)=<w,x>+b$$  \n",
    "函数间隔等于1的实例称为支持向量  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 几何间隔  \n",
    "几何间隔等于函数间隔的标准化  \n",
    "函数间隔能通过w进行缩放  \n",
    "当w是一个单位向量时,几何间隔等于函数间隔  \n",
    "最佳决策边界 : 具有最大几何间隔的决策边界(约束优化问题)  \n",
    "$$\\min\\frac{1}{2}<w,w>$$  \n",
    "$$s.t.  y_i(<w_i,x_i>+b)\\geq 1$$  \n",
    "凸优化问题,它的局部最小值也是全局最小值  \n",
    "表示为模型的对偶形式以适应核函数  \n",
    "$$W(\\alpha)=\\sum\\limits_i\\alpha_i-\\frac{1}{2}\\sum\\limits_{i,j}\\alpha_i\\alpha_jy_iy_jK(x_i,x_j)$$  \n",
    "$$s.t.   \\sum\\limits_{i=1}^ny_i\\alpha_i=0$$  \n",
    "$$s.t.   \\alpha_i\\leq 0$$  \n",
    "找出使几何间隔最大化的参数是一个二次规划问题,该问题通常使用序列最小优化算法(SMO)  \n",
    "SMO : 将优化问题分解成为一系列尽可能小的子问题,可以被分析解决"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T22:17:12.696442Z",
     "start_time": "2020-06-26T22:17:12.680684Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC,SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 人工神经网络(ANN)  \n",
    "ANN : 可用于监督学习和非监督任务(非线性模型)  \n",
    "ANN : 由人工神经元组成的有向图,图的边代表权重,这些权重都是模型需要学习的参数  \n",
    "模型的架构(拓扑) : 描述了神经元的类型和神经元之间的连接结构  \n",
    "激活函数  \n",
    "权重最优值的学习方法  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前馈神经网络  \n",
    "通过有向非循环图来定义  \n",
    "信息只在一个方向上朝着输出层进行传递  \n",
    "用于学习一个将输入映射到输出的函数  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 反馈神经网络(递归神经网络)  \n",
    "包含循环,表示网络的一种内部状态,它会导致网络的行为基于本身的输入随着时间变化而发生变化  \n",
    "用于处理输入序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 反向传播  \n",
    "计算一个神经网络的代价函数针对其每一个权重的梯度  \n",
    "反向传播算法是一种迭代算法,每次迭代包含两个阶段    \n",
    "第一阶段 : 向前传播(向前传递),输入通过网络的神经元层向前传播直到到达输出层,然后,损失函数可以用来计算预测的误差  \n",
    "第二阶段 : 向后传播阶段,误差从代价函数向输入传播以便每个神经元对于误差的贡献能够被估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 链式法则  \n",
    "用于计算两个或更多函数组合的导数  \n",
    "假设变量z依赖于y,y依赖于x,z针对x的导数  \n",
    "$$\\frac{dz}{dx}=\\frac{dz}{dy}\\cdot\\frac{dy}{dx}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T10:48:28.291765Z",
     "start_time": "2020-06-27T10:48:28.284102Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier,MLPRegressor # 多层感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主成分分析(PCA)  \n",
    "PCA(卡尔胡宁-勒夫转换KLT) : 在高维空间中发现模式的技术  \n",
    "降维可以压缩数据,将丢失的数据最小化  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T07:50:45.259328Z",
     "start_time": "2020-06-29T07:50:45.255340Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
